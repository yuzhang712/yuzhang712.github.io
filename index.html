<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="css/jemdoc.css" type="text/css" />
<link rel="stylesheet" href="css/myMiddle.css" type="text/css" />
<title>Yu Zhang - The Chinese University of Hong Kong</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="layout-content">
<table class="imgtable"><tr><td>
<img src="./yu.jpg" alt="" width="110px" />&nbsp;</td>
<td align="left"><h1>Yu Zhang</h1>
<p><b>Ph.D. student</b>  </p>
<p><a href="http://www.cse.cuhk.edu.hk" target=&ldquo;blank&rdquo;>Department of Computer Science and Engineering</a> <br />
<a href="http://www.cuhk.edu.hk/english/index.html" target=&ldquo;blank&rdquo;>The Chinese University of Hong Kong</a> <br />
<b>Email</b>:yuzhang712@link.cuhk.edu.hk <br /></p>

</td></tr></table>
<h2>Biography</h2>
<p>
  <!-- <b><font color="red">He is now looking for a PhD offer. <a href=mailto:werry715@gmail.com>contact him.</a> </font></b> -->
  <!-- <br /> -->
  <br />
    I am currently a final-year Ph.D. candidate at the Department of Computer Science and Engineering, 
  The Chinese University of Hong Kong (CUHK), under the supervision of Prof. Bei Yu. I also work as a research intern at
  NVIDIA’s Deep Learning Efficiency Research Group.
  My research interests are efficient LLM training/inference, software/hardware co-design, and LLM reasoning.

  Besides research, I love swimming and hiking in my spare time.
<br />

<h2>Research Directions</h2>
<ul>
  My research directions include:
  <li><p>
    LLM Efficiency and Reasoning: [C9, C10, C11]
  </p></li>
  <li><p>
    Mathematical Optimization: [C4, C6, C7, C8]
  </p></li>
  <li><p>
    Federated Learning: [C5, J1]
  </p></li>
  <li><p>
    Green Energy: [C1, C2, C3]
  </p></li>
</ul>

  

<h2>Publications</h2>
<h3>Conference Papers</h3>
<ul>
  <li><p>
    [C11] <b>Yu Zhang</b>, Hui-Ling Zhen, Mingxuan Yuan, Bei Yu, “MOSS: Efficient and Accurate FP8 LLM Training with Microscaling and Automatic Scaling”. International Conference on Learning Representations (<b>ICLR</b>), Rio de Janeiro, Brazil
  </p></li>
  <li><p>
    [C10] <b>Yu Zhang</b>, Mingzi Wang, Lancheng Zou, Wulong Liu, Hui-Ling Zhen, Mingxuan Yuan, Bei Yu, “MixPE: Quantization and Hardware Co-design for Efficient LLM Inference”. https://arxiv.org/pdf/2411.16158
  </p></li>
  <li><p>
    [C9] <b>Yu Zhang</b>, Hui-Ling Zhen, Zehua Pei, Yingzhao Lian, Lihao Yin, Mingxuan Yuan, Bei Yu, “DiLA: Enhancing LLM Tool Learning with Differential Logic Layer”, , SIGKDD Conference on Knowledge Discovery and Data Mining (<b>KDD</b>), Jeju, Korea, Aug. 9–13, 2026.
  </p></li>
  <li><p>
    [C8] <b>Yu Zhang</b>, Hui-Ling Zhen, Mingxuan Yuan, Bei Yu, “DiffSAT: Differential MaxSAT Layer for SAT Solving”, IEEE/ACM International Conference on Computer-Aided Design (<b>ICCAD</b>), New Jersey, Oct. 27–31, 2024.
  </p></li>
  <li><p>
    [C7] Yuan Pu, Fangzhou Liu, <b>Yu Zhang</b>, Zhuolun He, Kai-Yuan Chao, Yibo Lin, Bei Yu, “Lesyn: Placement-aware Logic Resynthesis for Non-Integer Multiple-Cell-Height Designs”, ACM/IEEE Design Automation Conference (<b>DAC</b>), San Francisco, Jun. 23–27, 2024.
  </p></li>
  <li><p>
    [C6] <b>Yu Zhang</b>, Yuan Pu, Fangzhou Liu, Peiyu Liao, Kai-Yuan Chao, Keren Zhu, Yibo Lin and Bei Yu, “Multi-Electrostatics Based Placement for Non-Integer Multiple-Height Cells”, International Symposium on Physical Design (<b>ISPD</b>), March 12–15, 2024.
  </p></li>
  <li><p>
    [C5] <b>Yu Zhang</b>, Wei Lin, Sisi Chen, Qingyu Song, Jiaxun Lu, Yunfeng Shao, Bei Yu and Hong Xu, “Fed2Com: Towards Efficient Compression in Federated Learning”, International Conference on Computing, Networking and Communications (<b>ICNC</b>), Feb. 19–22, 2024.
  </p></li>
  <li><p>
    [C4] <b>Yu Zhang</b>, Yifan Chen, Zhonglin Xie, Hong Xu, Zaiwen Wen, Yibo Lin and Bei Yu, “LRSDP: Low-rank SDP for Triple Patterning Lithography Layout Decomposition”, Design Automation Conference (<b>DAC</b>), July 09–13, 2023.
  </p></li>
  <li><p>
    [C3] Xudong Wang, Guoming Tang, Yi Wang, Srinivasan Keshav, <b>Yu Zhang</b>, “Evsense: A robust and
    scalable approach to non-intrusive ev charging detection”, ACM International Conference on Future
    Energy Systems (<b>e-Energy</b>).
  </p></li>
  <li><p>
    [C2] <b>Yu Zhang</b>, Guoming Tang, Qianyi Huang, Yi Wang, Hong Xu, Xudong Wang, “More Behind Your Electricity Bill: a Dual-DNN Approach to Non-Intrusive Load Monitoring”, IEEE International Conference on Cyber, Physical and Social Computing (<b>CPSCom</b>), Aug. 22–25, 2022.
  </p></li>
  <li><p>
    [C1] <b>Yu Zhang</b>, Guoming Tang, Qianyi Huang, Kui Wu, Yangjing Wu, Yi Wang, “Investigating Low-Battery Anxiety of Mobile Users”, IEEE International Conference on Cyber, Physical and Social Computing (<b>CPSCom</b>), Aug. 22–25, 2022.
  </p></li>
</ul>
<h3>Journal Papers</h3>
<ul>
  <li><p>
    [J1] <b>Yu Zhang</b>, Guoming Tang, Qianyi Huang, Yi Wang, Kui Wu, Keping Yu, Xun Shao, “FedNILM: Applying Federated Learning to NILM Applications at the Edge”, IEEE Transactions on Green Communications and Networking (<b>TGCN</b>), Apr. 14, 2022.
  </p></li>
</ul>
<h3>Patent</h3>
<ul>
  <li><p>
    “一种基于模型松弛的大模型新微调方法”, 92056131CN01.
  </p></li>
</ul>

<h2>Experiences</h2>
<ul>
  <li><p>
    Research Intern, NVIDIA Research, Santa Clara, U.S., Sep 2025 - Present 
    <br>
    Topic: Efficient Large Language Model, LLM Hallucination Detection
  </p></li>
  <li><p>
    Research Intern, Huawei Noah's Ark Lab, HKSAR, Aug 2023 - Aug 2024 
    <br>
    Topic: Hardware/software co-design for efficient LLM inference; Differential solver layer for LLM
  <li><p>
    Research Assistant, The Chinese University of Hong Kong, HKSAR, Nov 2021 - Jul 2022 
    <br>
    Topic: Communication-efficient and convergence fast federated learning
  </p></li>
  <li><p>
    Research Intern, Microsoft Research Asia, Beijing, P.R. China, Jan 2021 - Aug 2021
    <br>
    Topic: AISimulator
  </p></li>
  <!-- <li><p>
    Research Intern, Peng Cheng Laboratory, Shenzhen, P.R. China, Jun 2020 - Jan 2021
    <br>
    Topic: Fedrated Transfer Learning Framework for NILM
  </p></li>
  <li><p>
    Research Assistant, University of California, Berkeley, CA, U.S.A., Jul 2019 - Feb 2020
    <br>
    Topic: Highly Adaptive Lasso
  </p></li> -->
</ul>

<h2>Education</h2>
<ul>
  <li><p>
    Ph.D. Computer Science and Engineering, The Chinese University of Hong Kong, Aug 2022 - present
  </p></li>
  <li><p>
    B.S. Statistics, Central South University, Sept 2015 - Jun 2019
  </p></li>
</ul>

<h2>Professional Service</h2>
<h3>Journal Reviewer</h3>
<ul>
  <li><p>
    <b>TNNLS</b>, <b>TCAD</b>, <b>TODAES</b>, <b>TGCN</b>, <b>VLSIJ</b>
  </p></li>
  
</ul>
<h3>Conference Reviewer</h3>
<ul>
  <li><p>
    <b>ISPD</b>, <b>KDD</b>
  </p></li>
</ul>

<!-- <h2>Technical Skills</h2>
<ul>
  <li><p>
    <b>Deep Learning Frameworks</b>: PyTorch, JAX, Hugging Face Transformers
  </p></li>
  <li><p>
    <b>Deep Learning Frameworks</b>: PyTorch, JAX, Hugging Face Transformers
  </p></li>
</ul> -->

</html>
